{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Production-Grade Multivariate Seq2Seq LSTM for PJM Energy Forecasting\n",
        "\n",
        "**Author:** Senior Data Scientist - Energy Forecasting Specialist  \n",
        "**Purpose:** Day-Ahead Market Prediction (24-hour horizon from 168-hour lookback)\n",
        "\n",
        "## Architecture Overview\n",
        "- **Encoder:** Bidirectional LSTM capturing temporal patterns\n",
        "- **Decoder:** LSTM generating future sequences\n",
        "- **Features:** Engineered multivariate inputs from univariate time series\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Core Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Deep Learning\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, LSTM, Bidirectional, RepeatVector, \n",
        "    TimeDistributed, Dense, Dropout\n",
        ")\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Feature Engineering\n",
        "import holidays\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Configure plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (15, 6)\n",
        "\n",
        "print(f\"‚úÖ TensorFlow version: {tf.__version__}\")\n",
        "print(f\"‚úÖ GPU Available: {tf.config.list_physical_devices('GPU')}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "‚úÖ TensorFlow version: 2.20.0\n",
            "‚úÖ GPU Available: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Seq2SeqEnergyForecaster Class Definition\n",
        "\n",
        "This class encapsulates the entire ML pipeline:\n",
        "- Data loading and cleaning\n",
        "- Advanced feature engineering\n",
        "- Model architecture (Encoder-Decoder)\n",
        "- Training with callbacks\n",
        "- Rigorous evaluation with horizon analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Seq2SeqEnergyForecaster:\n",
        "    \"\"\"\n",
        "    Production-grade Seq2Seq LSTM forecaster for energy consumption.\n",
        "    \n",
        "    This class handles the complete ML pipeline:\n",
        "    - Data loading and cleaning\n",
        "    - Advanced feature engineering\n",
        "    - Model architecture (Encoder-Decoder)\n",
        "    - Training with callbacks\n",
        "    - Rigorous evaluation with horizon analysis\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, lookback=168, forecast_horizon=24):\n",
        "        \"\"\"\n",
        "        Initialize the forecaster.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        lookback : int\n",
        "            Number of past hours to use as input (default: 168 = 1 week)\n",
        "        forecast_horizon : int\n",
        "            Number of future hours to predict (default: 24 = 1 day)\n",
        "        \"\"\"\n",
        "        self.lookback = lookback\n",
        "        self.forecast_horizon = forecast_horizon\n",
        "        self.model = None\n",
        "        self.scaler_target = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.scaler_features = MinMaxScaler(feature_range=(0, 1))\n",
        "        self.feature_columns = []\n",
        "        self.us_holidays = holidays.US()\n",
        "        \n",
        "        print(f\"üîß Initialized Seq2Seq Forecaster\")\n",
        "        print(f\"   Lookback: {lookback} hours | Forecast: {forecast_horizon} hours\")\n",
        "    \n",
        "    \n",
        "    def load_and_clean_data(self, filepath):\n",
        "        \"\"\"\n",
        "        Load and clean the PJM energy consumption dataset.\n",
        "        \n",
        "        Steps:\n",
        "        1. Load CSV with datetime parsing\n",
        "        2. Sort by datetime\n",
        "        3. Remove duplicate timestamps\n",
        "        4. Resample to hourly frequency\n",
        "        5. Impute missing values via linear interpolation\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        filepath : str\n",
        "            Path to PJME_hourly.csv\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Cleaned dataframe with datetime index\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìÅ Loading data from {filepath}...\")\n",
        "        \n",
        "        # Load dataset\n",
        "        df = pd.read_csv(filepath)\n",
        "        \n",
        "        # Convert to datetime and set as index\n",
        "        df['Datetime'] = pd.to_datetime(df['Datetime'])\n",
        "        df = df.set_index('Datetime')\n",
        "        \n",
        "        # Sort by datetime\n",
        "        df = df.sort_index()\n",
        "        \n",
        "        # Remove duplicate timestamps (keep first occurrence)\n",
        "        duplicates_count = df.index.duplicated().sum()\n",
        "        if duplicates_count > 0:\n",
        "            print(f\"   ‚ö†Ô∏è  Found {duplicates_count} duplicate timestamps - removing...\")\n",
        "            df = df[~df.index.duplicated(keep='first')]\n",
        "        \n",
        "        # Resample to hourly frequency and interpolate missing values\n",
        "        df = df.asfreq('H')\n",
        "        missing_count = df['PJME_MW'].isna().sum()\n",
        "        if missing_count > 0:\n",
        "            print(f\"   ‚ö†Ô∏è  Found {missing_count} missing values - interpolating...\")\n",
        "            df['PJME_MW'] = df['PJME_MW'].interpolate(method='linear', limit_direction='both')\n",
        "        \n",
        "        print(f\"   ‚úÖ Data loaded: {len(df)} records from {df.index[0]} to {df.index[-1]}\")\n",
        "        \n",
        "        self.raw_data = df.copy()\n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def engineer_features(self, df):\n",
        "        \"\"\"\n",
        "        Create advanced multivariate features from univariate time series.\n",
        "        \n",
        "        Features Created:\n",
        "        -----------------\n",
        "        1. Cyclical Time Features (4 features):\n",
        "           - hour_sin, hour_cos: Capture daily periodicity\n",
        "           - month_sin, month_cos: Capture seasonal patterns\n",
        "           \n",
        "        2. Holiday Flag (1 feature):\n",
        "           - is_holiday: Binary indicator for US federal holidays\n",
        "           \n",
        "        3. Lag Features (3 features):\n",
        "           - lag_1h: Previous hour (short-term dependency)\n",
        "           - lag_24h: Same hour yesterday (daily pattern)\n",
        "           - lag_168h: Same hour last week (weekly pattern)\n",
        "        \n",
        "        Total: 8 features + 1 target = 9 columns\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            Cleaned dataframe with PJME_MW column\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            Feature-engineered dataframe\n",
        "        \"\"\"\n",
        "        print(f\"\\nüõ†Ô∏è  Engineering multivariate features...\")\n",
        "        \n",
        "        df = df.copy()\n",
        "        \n",
        "        # ==========================================\n",
        "        # 1. CYCLICAL TIME FEATURES\n",
        "        # ==========================================\n",
        "        # Hour of day (0-23) transformed to sin/cos to capture cyclical nature\n",
        "        # This prevents the model from treating hour 23 and hour 0 as distant\n",
        "        df['hour'] = df.index.hour\n",
        "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
        "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
        "        \n",
        "        # Month of year (1-12) for seasonal patterns\n",
        "        df['month'] = df.index.month\n",
        "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
        "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
        "        \n",
        "        # ==========================================\n",
        "        # 2. HOLIDAY FEATURE\n",
        "        # ==========================================\n",
        "        # US federal holidays often have different load patterns\n",
        "        df['is_holiday'] = df.index.to_series().apply(\n",
        "            lambda x: 1 if x.date() in self.us_holidays else 0\n",
        "        )\n",
        "        \n",
        "        # ==========================================\n",
        "        # 3. LAG FEATURES\n",
        "        # ==========================================\n",
        "        # These capture autoregressive dependencies at different time scales\n",
        "        \n",
        "        # Lag 1 hour: Immediate past (load tends to be smooth hour-to-hour)\n",
        "        df['lag_1h'] = df['PJME_MW'].shift(1)\n",
        "        \n",
        "        # Lag 24 hours: Daily pattern (same hour yesterday)\n",
        "        df['lag_24h'] = df['PJME_MW'].shift(24)\n",
        "        \n",
        "        # Lag 168 hours: Weekly pattern (same hour, same day of week)\n",
        "        df['lag_168h'] = df['PJME_MW'].shift(168)\n",
        "        \n",
        "        # Drop rows with NaN values created by lagging\n",
        "        # We lose the first 168 rows but gain powerful features\n",
        "        df = df.dropna()\n",
        "        \n",
        "        # Store feature column names (excluding temporary columns)\n",
        "        self.feature_columns = [\n",
        "            'hour_sin', 'hour_cos', 'month_sin', 'month_cos',\n",
        "            'is_holiday', 'lag_1h', 'lag_24h', 'lag_168h'\n",
        "        ]\n",
        "        \n",
        "        print(f\"   ‚úÖ Created {len(self.feature_columns)} features: {self.feature_columns}\")\n",
        "        print(f\"   üìä Final dataset shape: {df.shape}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    \n",
        "    def create_sequences(self, df, train_size=0.8):\n",
        "        \"\"\"\n",
        "        Create input-output sequences for Seq2Seq training.\n",
        "        \n",
        "        Architecture:\n",
        "        -------------\n",
        "        X shape: (samples, lookback=168, n_features=8)\n",
        "        y shape: (samples, forecast_horizon=24, 1)\n",
        "        \n",
        "        The model will learn to map a week of multivariate history\n",
        "        to a day of future energy consumption.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        df : pd.DataFrame\n",
        "            Feature-engineered dataframe\n",
        "        train_size : float\n",
        "            Fraction of data for training (default: 0.8)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        tuple\n",
        "            (X_train, y_train, X_test, y_test, test_dates)\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîÑ Creating Seq2Seq sequences...\")\n",
        "        \n",
        "        # Separate features and target\n",
        "        features = df[self.feature_columns].values\n",
        "        target = df['PJME_MW'].values.reshape(-1, 1)\n",
        "        \n",
        "        # Scale features and target separately\n",
        "        # This is crucial for proper inverse transformation later\n",
        "        features_scaled = self.scaler_features.fit_transform(features)\n",
        "        target_scaled = self.scaler_target.fit_transform(target)\n",
        "        \n",
        "        # Initialize lists to store sequences\n",
        "        X, y = [], []\n",
        "        dates = []\n",
        "        \n",
        "        # Create sliding window sequences\n",
        "        for i in range(self.lookback, len(df) - self.forecast_horizon + 1):\n",
        "            # Input: lookback hours of multivariate features\n",
        "            X.append(features_scaled[i - self.lookback:i])\n",
        "            \n",
        "            # Output: forecast_horizon hours of target variable\n",
        "            y.append(target_scaled[i:i + self.forecast_horizon])\n",
        "            \n",
        "            # Store the corresponding datetime for plotting\n",
        "            dates.append(df.index[i + self.forecast_horizon - 1])\n",
        "        \n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        dates = np.array(dates)\n",
        "        \n",
        "        # Split into train and test sets (temporal split - no shuffling!)\n",
        "        split_idx = int(len(X) * train_size)\n",
        "        \n",
        "        X_train = X[:split_idx]\n",
        "        y_train = y[:split_idx]\n",
        "        \n",
        "        X_test = X[split_idx:]\n",
        "        y_test = y[split_idx:]\n",
        "        test_dates = dates[split_idx:]\n",
        "        \n",
        "        print(f\"   ‚úÖ Sequences created:\")\n",
        "        print(f\"      X_train: {X_train.shape} | y_train: {y_train.shape}\")\n",
        "        print(f\"      X_test:  {X_test.shape}  | y_test:  {y_test.shape}\")\n",
        "        print(f\"      Train period: {df.index[self.lookback]} to {df.index[split_idx + self.lookback - 1]}\")\n",
        "        print(f\"      Test period:  {df.index[split_idx + self.lookback]} to {df.index[-1]}\")\n",
        "        \n",
        "        return X_train, y_train, X_test, y_test, test_dates\n",
        "    \n",
        "    \n",
        "    def build_model(self, n_features):\n",
        "        \"\"\"\n",
        "        Build Seq2Seq LSTM architecture using Functional API.\n",
        "        \n",
        "        Architecture Details:\n",
        "        ---------------------\n",
        "        1. Encoder: Bidirectional LSTM (64 units)\n",
        "           - Captures patterns in both forward and backward time\n",
        "           - Outputs: encoder_states (hidden + cell states)\n",
        "        \n",
        "        2. Bridge: RepeatVector(24)\n",
        "           - Repeats encoder output 24 times (one for each forecast hour)\n",
        "           - Prepares decoder input\n",
        "        \n",
        "        3. Decoder: LSTM (64 units, return_sequences=True)\n",
        "           - Generates the 24-hour forecast autoregressively\n",
        "        \n",
        "        4. Output: TimeDistributed Dense(1)\n",
        "           - Applies Dense layer to each of the 24 time steps\n",
        "           - Produces final load predictions\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        n_features : int\n",
        "            Number of input features\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        keras.Model\n",
        "            Compiled Seq2Seq model\n",
        "        \"\"\"\n",
        "        print(f\"\\nüèóÔ∏è  Building Seq2Seq LSTM architecture...\")\n",
        "        \n",
        "        # Input layer\n",
        "        encoder_inputs = Input(\n",
        "            shape=(self.lookback, n_features),\n",
        "            name='encoder_input'\n",
        "        )\n",
        "        \n",
        "        # ==========================================\n",
        "        # ENCODER: Bidirectional LSTM\n",
        "        # ==========================================\n",
        "        # Bidirectional allows the model to see future context in the input sequence\n",
        "        encoder = Bidirectional(\n",
        "            LSTM(64, return_state=True, name='encoder_lstm'),\n",
        "            name='bidirectional_encoder'\n",
        "        )\n",
        "        \n",
        "        # We only need the states, not the output sequence\n",
        "        # For Bidirectional LSTM, we get: output, fwd_h, fwd_c, bwd_h, bwd_c\n",
        "        encoder_outputs, fwd_h, fwd_c, bwd_h, bwd_c = encoder(encoder_inputs)\n",
        "        \n",
        "        # Concatenate forward and backward states\n",
        "        state_h = tf.keras.layers.Concatenate()([fwd_h, bwd_h])\n",
        "        state_c = tf.keras.layers.Concatenate()([fwd_c, bwd_c])\n",
        "        \n",
        "        # ==========================================\n",
        "        # BRIDGE: Repeat Vector\n",
        "        # ==========================================\n",
        "        # Repeat the final encoder state for each forecast time step\n",
        "        repeated_context = RepeatVector(self.forecast_horizon)(state_h)\n",
        "        \n",
        "        # ==========================================\n",
        "        # DECODER: LSTM\n",
        "        # ==========================================\n",
        "        # Generate the 24-hour forecast sequence\n",
        "        decoder_lstm = LSTM(\n",
        "            128,  # Match the concatenated state size (64*2)\n",
        "            return_sequences=True,\n",
        "            name='decoder_lstm'\n",
        "        )(repeated_context, initial_state=[state_h, state_c])\n",
        "        \n",
        "        # Add dropout for regularization\n",
        "        decoder_dropout = Dropout(0.2, name='decoder_dropout')(decoder_lstm)\n",
        "        \n",
        "        # ==========================================\n",
        "        # OUTPUT: Time Distributed Dense\n",
        "        # ==========================================\n",
        "        # Apply Dense layer independently to each time step\n",
        "        decoder_outputs = TimeDistributed(\n",
        "            Dense(1, activation='linear'),\n",
        "            name='output_layer'\n",
        "        )(decoder_dropout)\n",
        "        \n",
        "        # Create the model\n",
        "        model = Model(encoder_inputs, decoder_outputs, name='Seq2Seq_Energy_Forecaster')\n",
        "        \n",
        "        # Compile with Adam optimizer\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=0.001),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "        \n",
        "        print(f\"   ‚úÖ Model built successfully!\")\n",
        "        model.summary()\n",
        "        \n",
        "        self.model = model\n",
        "        return model\n",
        "    \n",
        "    \n",
        "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
        "              epochs=100, batch_size=32, verbose=1):\n",
        "        \"\"\"\n",
        "        Train the Seq2Seq model with callbacks.\n",
        "        \n",
        "        Callbacks:\n",
        "        ----------\n",
        "        1. EarlyStopping: Stop if validation loss doesn't improve for 15 epochs\n",
        "        2. ReduceLROnPlateau: Reduce learning rate if loss plateaus\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_train, y_train : np.ndarray\n",
        "            Training sequences\n",
        "        X_val, y_val : np.ndarray, optional\n",
        "            Validation sequences (if None, uses 20% of training data)\n",
        "        epochs : int\n",
        "            Maximum training epochs\n",
        "        batch_size : int\n",
        "            Batch size for training\n",
        "        verbose : int\n",
        "            Verbosity level (0, 1, or 2)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        keras.History\n",
        "            Training history\n",
        "        \"\"\"\n",
        "        print(f\"\\nüöÄ Training Seq2Seq model...\")\n",
        "        \n",
        "        # Define callbacks\n",
        "        callbacks = [\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=15,\n",
        "                restore_best_weights=True,\n",
        "                verbose=1\n",
        "            ),\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=5,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        # Determine validation data\n",
        "        if X_val is None or y_val is None:\n",
        "            validation_split = 0.2\n",
        "            validation_data = None\n",
        "            print(f\"   Using 20% of training data for validation\")\n",
        "        else:\n",
        "            validation_split = 0.0\n",
        "            validation_data = (X_val, y_val)\n",
        "            print(f\"   Using provided validation set: {X_val.shape}\")\n",
        "        \n",
        "        # Train the model\n",
        "        history = self.model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_split=validation_split,\n",
        "            validation_data=validation_data,\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            callbacks=callbacks,\n",
        "            verbose=verbose\n",
        "        )\n",
        "        \n",
        "        print(f\"   ‚úÖ Training completed!\")\n",
        "        print(f\"      Best val_loss: {min(history.history['val_loss']):.4f}\")\n",
        "        print(f\"      Final val_loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "        \n",
        "        self.history = history\n",
        "        return history\n",
        "    \n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Generate predictions and inverse transform to original scale.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X : np.ndarray\n",
        "            Input sequences (scaled)\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        np.ndarray\n",
        "            Predictions in original MW scale\n",
        "        \"\"\"\n",
        "        # Get scaled predictions\n",
        "        y_pred_scaled = self.model.predict(X, verbose=0)\n",
        "        \n",
        "        # Reshape for inverse transform\n",
        "        # From (samples, 24, 1) to (samples * 24, 1)\n",
        "        y_pred_reshaped = y_pred_scaled.reshape(-1, 1)\n",
        "        \n",
        "        # Inverse transform to original scale\n",
        "        y_pred_original = self.scaler_target.inverse_transform(y_pred_reshaped)\n",
        "        \n",
        "        # Reshape back to (samples, 24, 1)\n",
        "        y_pred = y_pred_original.reshape(-1, self.forecast_horizon, 1)\n",
        "        \n",
        "        return y_pred\n",
        "    \n",
        "    \n",
        "    def evaluate(self, X_test, y_test, test_dates):\n",
        "        \"\"\"\n",
        "        Rigorous evaluation with multiple metrics and visualizations.\n",
        "        \n",
        "        Metrics:\n",
        "        --------\n",
        "        1. Overall RMSE and MAPE\n",
        "        2. Per-hour RMSE (forecast horizon analysis)\n",
        "        \n",
        "        Visualizations:\n",
        "        ---------------\n",
        "        1. Forecast horizon degradation plot\n",
        "        2. Actual vs Predicted for sample week\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        X_test, y_test : np.ndarray\n",
        "            Test sequences (y_test is scaled)\n",
        "        test_dates : np.ndarray\n",
        "            Datetime index for test set\n",
        "            \n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Evaluation metrics\n",
        "        \"\"\"\n",
        "        print(f\"\\nüìä Evaluating model on test set...\")\n",
        "        \n",
        "        # Generate predictions (already in original scale)\n",
        "        y_pred = self.predict(X_test)\n",
        "        \n",
        "        # Inverse transform y_test to original scale\n",
        "        y_test_reshaped = y_test.reshape(-1, 1)\n",
        "        y_test_original = self.scaler_target.inverse_transform(y_test_reshaped)\n",
        "        y_test_original = y_test_original.reshape(-1, self.forecast_horizon, 1)\n",
        "        \n",
        "        # Flatten for overall metrics\n",
        "        y_test_flat = y_test_original.flatten()\n",
        "        y_pred_flat = y_pred.flatten()\n",
        "        \n",
        "        # ==========================================\n",
        "        # 1. OVERALL METRICS\n",
        "        # ==========================================\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_flat, y_pred_flat))\n",
        "        mae = mean_absolute_error(y_test_flat, y_pred_flat)\n",
        "        \n",
        "        # MAPE (avoid division by zero)\n",
        "        mape = np.mean(np.abs((y_test_flat - y_pred_flat) / (y_test_flat + 1e-10))) * 100\n",
        "        \n",
        "        print(f\"\\n   üìà Overall Performance:\")\n",
        "        print(f\"      RMSE: {rmse:.2f} MW\")\n",
        "        print(f\"      MAE:  {mae:.2f} MW\")\n",
        "        print(f\"      MAPE: {mape:.2f}%\")\n",
        "        \n",
        "        # ==========================================\n",
        "        # 2. PER-HOUR RMSE (Horizon Analysis)\n",
        "        # ==========================================\n",
        "        print(f\"\\n   üéØ Forecast Horizon Analysis (RMSE per hour):\")\n",
        "        \n",
        "        rmse_per_hour = []\n",
        "        for hour in range(self.forecast_horizon):\n",
        "            y_true_hour = y_test_original[:, hour, 0]\n",
        "            y_pred_hour = y_pred[:, hour, 0]\n",
        "            rmse_hour = np.sqrt(mean_squared_error(y_true_hour, y_pred_hour))\n",
        "            rmse_per_hour.append(rmse_hour)\n",
        "            \n",
        "            if hour < 6 or hour >= self.forecast_horizon - 3:\n",
        "                print(f\"      Hour t+{hour+1:2d}: {rmse_hour:.2f} MW\")\n",
        "            elif hour == 6:\n",
        "                print(f\"      ...\")\n",
        "        \n",
        "        # ==========================================\n",
        "        # 3. VISUALIZATIONS\n",
        "        # ==========================================\n",
        "        fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
        "        \n",
        "        # Plot 1: Forecast Horizon Degradation\n",
        "        axes[0].plot(range(1, self.forecast_horizon + 1), rmse_per_hour, \n",
        "                     marker='o', linewidth=2, markersize=6, color='#E74C3C')\n",
        "        axes[0].set_xlabel('Forecast Hour (t+1 to t+24)', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_ylabel('RMSE (MW)', fontsize=12, fontweight='bold')\n",
        "        axes[0].set_title('Forecast Accuracy Degradation Over 24-Hour Horizon', \n",
        "                         fontsize=14, fontweight='bold', pad=20)\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "        axes[0].set_xticks(range(1, self.forecast_horizon + 1))\n",
        "        \n",
        "        # Add annotation for best and worst hours\n",
        "        best_hour = np.argmin(rmse_per_hour) + 1\n",
        "        worst_hour = np.argmax(rmse_per_hour) + 1\n",
        "        axes[0].annotate(f'Best: t+{best_hour}\\n({min(rmse_per_hour):.1f} MW)', \n",
        "                        xy=(best_hour, min(rmse_per_hour)),\n",
        "                        xytext=(best_hour, min(rmse_per_hour) - 50),\n",
        "                        arrowprops=dict(arrowstyle='->', color='green', lw=2),\n",
        "                        fontsize=10, ha='center', color='green', fontweight='bold')\n",
        "        axes[0].annotate(f'Worst: t+{worst_hour}\\n({max(rmse_per_hour):.1f} MW)', \n",
        "                        xy=(worst_hour, max(rmse_per_hour)),\n",
        "                        xytext=(worst_hour, max(rmse_per_hour) + 50),\n",
        "                        arrowprops=dict(arrowstyle='->', color='red', lw=2),\n",
        "                        fontsize=10, ha='center', color='red', fontweight='bold')\n",
        "        \n",
        "        # Plot 2: Actual vs Predicted for a Sample Week\n",
        "        # Select a week from the middle of the test set\n",
        "        week_start_idx = len(test_dates) // 2\n",
        "        week_end_idx = week_start_idx + 7 * 24  # 7 days\n",
        "        \n",
        "        if week_end_idx <= len(test_dates):\n",
        "            # Create continuous timeline for the week\n",
        "            week_hours = []\n",
        "            week_actual = []\n",
        "            week_predicted = []\n",
        "            \n",
        "            for i in range(week_start_idx, min(week_end_idx, len(test_dates))):\n",
        "                # For each forecast, we only plot the first hour to avoid overlap\n",
        "                week_hours.append(test_dates[i])\n",
        "                week_actual.append(y_test_original[i, 0, 0])\n",
        "                week_predicted.append(y_pred[i, 0, 0])\n",
        "            \n",
        "            axes[1].plot(week_hours, week_actual, label='Actual', \n",
        "                        linewidth=2, alpha=0.8, color='#3498DB')\n",
        "            axes[1].plot(week_hours, week_predicted, label='Predicted', \n",
        "                        linewidth=2, alpha=0.8, color='#E67E22', linestyle='--')\n",
        "            axes[1].set_xlabel('Date & Time', fontsize=12, fontweight='bold')\n",
        "            axes[1].set_ylabel('Energy Consumption (MW)', fontsize=12, fontweight='bold')\n",
        "            axes[1].set_title(f'Actual vs Predicted: Sample Week in Test Set', \n",
        "                             fontsize=14, fontweight='bold', pad=20)\n",
        "            axes[1].legend(fontsize=11, loc='upper right')\n",
        "            axes[1].grid(True, alpha=0.3)\n",
        "            \n",
        "            # Rotate x-axis labels for better readability\n",
        "            plt.setp(axes[1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Return metrics dictionary\n",
        "        metrics = {\n",
        "            'rmse': rmse,\n",
        "            'mae': mae,\n",
        "            'mape': mape,\n",
        "            'rmse_per_hour': rmse_per_hour,\n",
        "            'degradation_rate': (rmse_per_hour[-1] - rmse_per_hour[0]) / rmse_per_hour[0] * 100\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n   üìâ Degradation Analysis:\")\n",
        "        print(f\"      RMSE at t+1:  {rmse_per_hour[0]:.2f} MW\")\n",
        "        print(f\"      RMSE at t+24: {rmse_per_hour[-1]:.2f} MW\")\n",
        "        print(f\"      Degradation:  {metrics['degradation_rate']:.1f}% increase\")\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    \n",
        "    def plot_training_history(self):\n",
        "        \"\"\"\n",
        "        Plot training and validation loss curves.\n",
        "        \"\"\"\n",
        "        if not hasattr(self, 'history'):\n",
        "            print(\"‚ö†Ô∏è  No training history available. Train the model first.\")\n",
        "            return\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(12, 5))\n",
        "        \n",
        "        epochs = range(1, len(self.history.history['loss']) + 1)\n",
        "        \n",
        "        ax.plot(epochs, self.history.history['loss'], \n",
        "                label='Training Loss', linewidth=2, color='#3498DB')\n",
        "        ax.plot(epochs, self.history.history['val_loss'], \n",
        "                label='Validation Loss', linewidth=2, color='#E74C3C')\n",
        "        ax.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
        "        ax.set_ylabel('MSE Loss', fontsize=12, fontweight='bold')\n",
        "        ax.set_title('Training & Validation Loss Over Time', \n",
        "                    fontsize=14, fontweight='bold', pad=20)\n",
        "        ax.legend(fontsize=11)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    \n",
        "    \n",
        "    def save_model(self, filepath='seq2seq_energy_model.h5'):\n",
        "        \"\"\"\n",
        "        Save the trained model to disk.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        filepath : str\n",
        "            Path to save the model\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            print(\"‚ö†Ô∏è  No model to save. Build and train a model first.\")\n",
        "            return\n",
        "        \n",
        "        self.model.save(filepath)\n",
        "        print(f\"üíæ Model saved to: {filepath}\")\n",
        "    \n",
        "    \n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Load a trained model from disk.\n",
        "        \n",
        "        Parameters:\n",
        "        -----------\n",
        "        filepath : str\n",
        "            Path to the saved model\n",
        "        \"\"\"\n",
        "        self.model = keras.models.load_model(filepath)\n",
        "        print(f\"üìÇ Model loaded from: {filepath}\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Initialize Forecaster\n",
        "\n",
        "Create a forecaster instance with:\n",
        "- **Lookback window:** 168 hours (1 week of historical data)\n",
        "- **Forecast horizon:** 24 hours (day-ahead prediction)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "forecaster = Seq2SeqEnergyForecaster(\n",
        "    lookback=168,        # 1 week of history\n",
        "    forecast_horizon=24  # Predict next 24 hours\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "üîß Initialized Seq2Seq Forecaster\n",
            "   Lookback: 168 hours | Forecast: 24 hours\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load and Clean Data\n",
        "\n",
        "Load the PJM hourly energy consumption dataset and perform cleaning:\n",
        "- Remove duplicate timestamps\n",
        "- Resample to hourly frequency\n",
        "- Interpolate missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load the dataset\n",
        "df = forecaster.load_and_clean_data('PJME_hourly.csv')\n",
        "\n",
        "# Display basic statistics\n",
        "print(\"\\nüìä Dataset Statistics:\")\n",
        "print(df['PJME_MW'].describe())\n",
        "\n",
        "# Plot the raw time series\n",
        "plt.figure(figsize=(15, 5))\n",
        "plt.plot(df.index, df['PJME_MW'], linewidth=0.5, alpha=0.7)\n",
        "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Energy Consumption (MW)', fontsize=12, fontweight='bold')\n",
        "plt.title('PJM Hourly Energy Consumption - Raw Time Series', fontsize=14, fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "üìÅ Loading data from PJME_hourly.csv...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'PJME_hourly.csv'",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mforecaster\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_and_clean_data\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPJME_hourly.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Display basic statistics\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìä Dataset Statistics:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mSeq2SeqEnergyForecaster.load_and_clean_data\u001b[39m\u001b[34m(self, filepath)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìÅ Loading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Convert to datetime and set as index\u001b[39;00m\n\u001b[32m     63\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33mDatetime\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df[\u001b[33m'\u001b[39m\u001b[33mDatetime\u001b[39m\u001b[33m'\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marri\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'PJME_hourly.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Engineering\n",
        "\n",
        "Transform the univariate time series into multivariate features:\n",
        "- **Cyclical features:** Hour and month (sine/cosine encoding)\n",
        "- **Holiday flag:** US federal holidays\n",
        "- **Lag features:** 1h, 24h, 168h lags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Engineer features\n",
        "df_features = forecaster.engineer_features(df)\n",
        "\n",
        "# Display feature correlations\n",
        "print(\"\\nüîó Feature Correlations with Target (PJME_MW):\")\n",
        "correlations = df_features[forecaster.feature_columns + ['PJME_MW']].corr()['PJME_MW'].sort_values(ascending=False)\n",
        "print(correlations)\n",
        "\n",
        "# Visualize feature distributions\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(forecaster.feature_columns):\n",
        "    axes[idx].hist(df_features[col], bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_title(col, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Value')\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Sequences\n",
        "\n",
        "Transform the data into sequences suitable for Seq2Seq learning:\n",
        "- **Input:** 168 hours √ó 8 features\n",
        "- **Output:** 24 hours √ó 1 target\n",
        "- **Split:** 80% train, 20% test (temporal split)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create sequences\n",
        "X_train, y_train, X_test, y_test, test_dates = forecaster.create_sequences(\n",
        "    df_features, \n",
        "    train_size=0.8\n",
        ")\n",
        "\n",
        "# Visualize a sample sequence\n",
        "sample_idx = 0\n",
        "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
        "\n",
        "# Plot input sequence (first 3 features for clarity)\n",
        "for i in range(3):\n",
        "    axes[0].plot(range(168), X_train[sample_idx, :, i], label=forecaster.feature_columns[i], alpha=0.7)\n",
        "axes[0].set_xlabel('Hour (Lookback Window)', fontweight='bold')\n",
        "axes[0].set_ylabel('Scaled Value', fontweight='bold')\n",
        "axes[0].set_title('Sample Input Sequence (First 3 Features)', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot output sequence (target)\n",
        "axes[1].plot(range(24), y_train[sample_idx, :, 0], marker='o', linewidth=2, markersize=4, color='#E74C3C')\n",
        "axes[1].set_xlabel('Hour (Forecast Horizon)', fontweight='bold')\n",
        "axes[1].set_ylabel('Scaled Load', fontweight='bold')\n",
        "axes[1].set_title('Sample Output Sequence (Target)', fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Build Model\n",
        "\n",
        "Construct the Seq2Seq LSTM architecture:\n",
        "- **Encoder:** Bidirectional LSTM (64 units)\n",
        "- **Bridge:** RepeatVector (24 timesteps)\n",
        "- **Decoder:** LSTM (128 units) + Dropout (0.2)\n",
        "- **Output:** TimeDistributed Dense (1 unit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build the model\n",
        "n_features = X_train.shape[2]\n",
        "model = forecaster.build_model(n_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Train Model\n",
        "\n",
        "Train the Seq2Seq model with:\n",
        "- **Optimizer:** Adam (lr=0.001)\n",
        "- **Loss:** Mean Squared Error (MSE)\n",
        "- **Callbacks:** EarlyStopping, ReduceLROnPlateau\n",
        "- **Epochs:** Up to 100 (early stopping will trigger earlier)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true
      },
      "source": [
        "# Train the model\n",
        "history = forecaster.train(\n",
        "    X_train, y_train,\n",
        "    epochs=100,\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Plot Training History\n",
        "\n",
        "Visualize the training and validation loss curves to assess convergence and overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Plot training history\n",
        "forecaster.plot_training_history()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Evaluate Model\n",
        "\n",
        "### Comprehensive Evaluation:\n",
        "1. **Overall Metrics:** RMSE, MAE, MAPE\n",
        "2. **Horizon Analysis:** Per-hour RMSE (t+1 to t+24)\n",
        "3. **Visual Inspection:** Actual vs Predicted for sample week\n",
        "\n",
        "This is the **critical innovation** - quantifying how forecast accuracy degrades with horizon."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluate on test set\n",
        "metrics = forecaster.evaluate(X_test, y_test, test_dates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Detailed Metrics Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Print comprehensive metrics\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL EVALUATION SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nüìä Overall Performance Metrics:\")\n",
        "print(f\"   ‚Ä¢ RMSE (Root Mean Squared Error): {metrics['rmse']:.2f} MW\")\n",
        "print(f\"   ‚Ä¢ MAE (Mean Absolute Error):      {metrics['mae']:.2f} MW\")\n",
        "print(f\"   ‚Ä¢ MAPE (Mean Abs % Error):        {metrics['mape']:.2f}%\")\n",
        "\n",
        "print(f\"\\nüéØ Forecast Horizon Performance:\")\n",
        "print(f\"   ‚Ä¢ Best Hour RMSE:    t+{np.argmin(metrics['rmse_per_hour'])+1} = {min(metrics['rmse_per_hour']):.2f} MW\")\n",
        "print(f\"   ‚Ä¢ Worst Hour RMSE:   t+{np.argmax(metrics['rmse_per_hour'])+1} = {max(metrics['rmse_per_hour']):.2f} MW\")\n",
        "print(f\"   ‚Ä¢ RMSE at t+1:       {metrics['rmse_per_hour'][0]:.2f} MW\")\n",
        "print(f\"   ‚Ä¢ RMSE at t+12:      {metrics['rmse_per_hour'][11]:.2f} MW\")\n",
        "print(f\"   ‚Ä¢ RMSE at t+24:      {metrics['rmse_per_hour'][-1]:.2f} MW\")\n",
        "print(f\"   ‚Ä¢ Degradation Rate:  {metrics['degradation_rate']:.1f}%\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Advanced Analysis: Error Distribution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate predictions for error analysis\n",
        "y_pred = forecaster.predict(X_test)\n",
        "\n",
        "# Inverse transform y_test\n",
        "y_test_reshaped = y_test.reshape(-1, 1)\n",
        "y_test_original = forecaster.scaler_target.inverse_transform(y_test_reshaped)\n",
        "y_test_original = y_test_original.reshape(-1, 24, 1)\n",
        "\n",
        "# Calculate errors\n",
        "errors = y_test_original.flatten() - y_pred.flatten()\n",
        "\n",
        "# Plot error distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Histogram\n",
        "axes[0].hist(errors, bins=100, edgecolor='black', alpha=0.7, color='#3498DB')\n",
        "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
        "axes[0].set_xlabel('Prediction Error (MW)', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Error Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Q-Q Plot\n",
        "from scipy import stats\n",
        "stats.probplot(errors, dist=\"norm\", plot=axes[1])\n",
        "axes[1].set_title('Q-Q Plot (Normal Distribution)', fontsize=14, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Error statistics\n",
        "print(f\"\\nüìâ Error Statistics:\")\n",
        "print(f\"   ‚Ä¢ Mean Error:      {np.mean(errors):.2f} MW (bias)\")\n",
        "print(f\"   ‚Ä¢ Std Error:       {np.std(errors):.2f} MW\")\n",
        "print(f\"   ‚Ä¢ Skewness:        {stats.skew(errors):.3f}\")\n",
        "print(f\"   ‚Ä¢ Kurtosis:        {stats.kurtosis(errors):.3f}\")\n",
        "print(f\"   ‚Ä¢ Min Error:       {np.min(errors):.2f} MW\")\n",
        "print(f\"   ‚Ä¢ Max Error:       {np.max(errors):.2f} MW\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Prediction Intervals (95% Confidence)\n",
        "\n",
        "Approximate prediction intervals using the RMSE as a proxy for standard error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select a random sample for visualization\n",
        "sample_idx = np.random.randint(0, len(X_test))\n",
        "\n",
        "# Get prediction and actual\n",
        "y_pred_sample = y_pred[sample_idx, :, 0]\n",
        "y_actual_sample = y_test_original[sample_idx, :, 0]\n",
        "\n",
        "# Calculate 95% confidence intervals (assuming normal distribution)\n",
        "z_score = 1.96  # 95% confidence\n",
        "upper_bound = y_pred_sample + z_score * np.array(metrics['rmse_per_hour'])\n",
        "lower_bound = y_pred_sample - z_score * np.array(metrics['rmse_per_hour'])\n",
        "\n",
        "# Plot\n",
        "hours = range(1, 25)\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(hours, y_actual_sample, label='Actual', marker='o', linewidth=2, color='#3498DB')\n",
        "plt.plot(hours, y_pred_sample, label='Predicted', marker='s', linewidth=2, color='#E67E22')\n",
        "plt.fill_between(hours, lower_bound, upper_bound, alpha=0.3, color='#E67E22', label='95% Confidence Interval')\n",
        "plt.xlabel('Forecast Hour', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Energy Consumption (MW)', fontsize=12, fontweight='bold')\n",
        "plt.title(f'24-Hour Forecast with Prediction Intervals\\nDate: {test_dates[sample_idx]}', \n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.xticks(hours)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate coverage\n",
        "within_interval = np.sum((y_test_original.flatten() >= (y_pred.flatten() - z_score * metrics['rmse'])) & \n",
        "                         (y_test_original.flatten() <= (y_pred.flatten() + z_score * metrics['rmse'])))\n",
        "coverage = within_interval / len(y_test_original.flatten()) * 100\n",
        "print(f\"\\nüìä Prediction Interval Coverage: {coverage:.1f}% (Target: 95%)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Save Model\n",
        "\n",
        "Save the trained model for future use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Save the model\n",
        "forecaster.save_model('seq2seq_energy_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 15. Inference Example\n",
        "\n",
        "Demonstrate how to use the trained model for new predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Select a random test sample\n",
        "random_idx = np.random.randint(0, len(X_test))\n",
        "\n",
        "# Get input sequence\n",
        "input_sequence = X_test[random_idx:random_idx+1]\n",
        "forecast_date = test_dates[random_idx]\n",
        "\n",
        "# Predict\n",
        "prediction = forecaster.predict(input_sequence)\n",
        "\n",
        "print(f\"\\nüîÆ Forecast for: {forecast_date}\")\n",
        "print(f\"\\nPredicted 24-hour load profile (MW):\")\n",
        "print(\"-\" * 50)\n",
        "for hour in range(24):\n",
        "    load_mw = prediction[0, hour, 0]\n",
        "    hour_of_day = (forecast_date.hour + hour + 1) % 24\n",
        "    print(f\"  Hour {hour+1:2d} ({hour_of_day:02d}:00): {load_mw:,.0f} MW\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 16. Feature Importance Analysis\n",
        "\n",
        "Analyze which features contribute most to predictions using permutation importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simplified feature importance via permutation\n",
        "# Note: This is a basic approximation; full implementation would require more sophisticated methods\n",
        "\n",
        "baseline_rmse = metrics['rmse']\n",
        "importance_scores = {}\n",
        "\n",
        "print(\"\\nüîç Feature Importance Analysis (via Permutation):\")\n",
        "print(\"   Computing... (this may take a moment)\\n\")\n",
        "\n",
        "for feat_idx, feat_name in enumerate(forecaster.feature_columns):\n",
        "    # Create a copy of test set\n",
        "    X_permuted = X_test.copy()\n",
        "    \n",
        "    # Permute (shuffle) this feature across all samples\n",
        "    X_permuted[:, :, feat_idx] = np.random.permutation(X_permuted[:, :, feat_idx].flatten()).reshape(X_permuted[:, :, feat_idx].shape)\n",
        "    \n",
        "    # Get predictions with permuted feature\n",
        "    y_pred_permuted = forecaster.predict(X_permuted)\n",
        "    \n",
        "    # Calculate RMSE\n",
        "    rmse_permuted = np.sqrt(mean_squared_error(y_test_original.flatten(), y_pred_permuted.flatten()))\n",
        "    \n",
        "    # Importance = increase in error\n",
        "    importance = rmse_permuted - baseline_rmse\n",
        "    importance_scores[feat_name] = importance\n",
        "    \n",
        "    print(f\"   ‚Ä¢ {feat_name:12s}: {importance:+.2f} MW (RMSE increase when permuted)\")\n",
        "\n",
        "# Visualize\n",
        "sorted_features = sorted(importance_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "features, scores = zip(*sorted_features)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "colors = ['#E74C3C' if s > 0 else '#3498DB' for s in scores]\n",
        "plt.barh(features, scores, color=colors, edgecolor='black', alpha=0.7)\n",
        "plt.xlabel('RMSE Increase when Permuted (MW)', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Feature', fontsize=12, fontweight='bold')\n",
        "plt.title('Feature Importance (Permutation Method)', fontsize=14, fontweight='bold')\n",
        "plt.axvline(0, color='black', linestyle='--', linewidth=1)\n",
        "plt.grid(True, alpha=0.3, axis='x')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 17. Conclusion & Next Steps\n",
        "\n",
        "### ‚úÖ What We've Accomplished:\n",
        "- Built production-grade Seq2Seq LSTM for day-ahead energy forecasting\n",
        "- Engineered 8 features from univariate time series\n",
        "- Achieved competitive RMSE and MAPE metrics\n",
        "- Quantified forecast horizon degradation\n",
        "- Provided comprehensive evaluation and visualizations\n",
        "\n",
        "### üöÄ Potential Improvements:\n",
        "1. **Add Weather Data:** Temperature forecasts would significantly improve summer predictions\n",
        "2. **Probabilistic Forecasting:** Implement quantile regression for prediction intervals\n",
        "3. **Attention Mechanism:** Add attention to identify which historical hours matter most\n",
        "4. **Ensemble Methods:** Combine with XGBoost or Prophet for robustness\n",
        "5. **Transfer Learning:** Pre-train on multiple ISOs, fine-tune on PJM\n",
        "6. **Online Learning:** Implement incremental updates as new data arrives\n",
        "\n",
        "### üì¶ Deployment Checklist:\n",
        "- [ ] Set up automated daily retraining pipeline\n",
        "- [ ] Implement monitoring dashboard (track RMSE drift)\n",
        "- [ ] Create API endpoint for real-time inference\n",
        "- [ ] Establish data quality checks\n",
        "- [ ] Document edge cases and failure modes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nFinal Model Performance:\")\n",
        "print(f\"  ‚Ä¢ RMSE: {metrics['rmse']:.2f} MW\")\n",
        "print(f\"  ‚Ä¢ MAPE: {metrics['mape']:.2f}%\")\n",
        "print(f\"  ‚Ä¢ Degradation: {metrics['degradation_rate']:.1f}%\")\n",
        "print(\"\\nModel saved to: seq2seq_energy_model.h5\")\n",
        "print(\"=\"*80)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}